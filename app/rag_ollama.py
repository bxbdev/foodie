import os
from pathlib import Path
from dotenv import load_dotenv
from llama_index.core import SimpleDirectoryReader, Settings, VectorStoreIndex, StorageContext, load_index_from_storage
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.core.prompts import PromptTemplate
from file_monitor import FileMonitor  # åŒ¯å…¥æˆ‘å€‘çš„æª”æ¡ˆç›£æ§ç³»çµ±

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")  # é è¨­å€¼ä½œç‚º fallback 

models = [
    "gemma3:27b",
    "codellama:34b",
    "llama3.2-vision:11b",
]

Settings.llm = Ollama(
    model=models[0],
    base_url=OLLAMA_URL,
    request_timeout=120.0,
)
Settings.embed_model = OllamaEmbedding(
    model_name="nomic-embed-text:latest",
    base_url=OLLAMA_URL
)

# vec = Settings.embed_model.get_text_embedding("hello world")
# print(len(vec), vec[:5])

# è¨­å®šç›®éŒ„è·¯å¾‘
BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
persist_dir = BASE_DIR / "storage"

qa_tmpl = PromptTemplate(
    "ä½ æ˜¯åš´è¬¹çš„ä¼æ¥­åŠ©ç†ã€‚åªæ ¹æ“šæä¾›çš„å…§å®¹å›ç­”"
    "\n\n[å…§å®¹]\n{context_str}\n\n[å•é¡Œ]\n{query_str}"
    "\n\nè‹¥å…§å®¹ä¸è¶³ï¼Œè«‹å›ç­”ï¼šä¸çŸ¥é“ã€‚"
)

def create_or_load_index():
    """
    æ™ºèƒ½ç´¢å¼•ç®¡ç†å‡½æ•¸ï¼šæ ¹æ“šæª”æ¡ˆè®ŠåŒ–æ±ºå®šæ˜¯å»ºç«‹æ–°ç´¢å¼•é‚„æ˜¯è¼‰å…¥ç¾æœ‰ç´¢å¼•
    """
    print("=== æ™ºèƒ½ RAG ç´¢å¼•ç®¡ç†ç³»çµ± ===")
    
    # 1. å»ºç«‹æª”æ¡ˆç›£æ§å™¨
    monitor = FileMonitor(DATA_DIR)
    
    # 2. æª¢æŸ¥æ˜¯å¦å­˜åœ¨èˆŠçš„ç´¢å¼•
    index_exists = persist_dir.exists() and any(persist_dir.iterdir())
    
    # 3. æª¢æŸ¥æª”æ¡ˆæ˜¯å¦æœ‰è®ŠåŒ–
    has_changes, changed_files = monitor.has_changes()
    
    # 4. æ±ºå®šæ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•
    need_rebuild = not index_exists or has_changes
    
    if need_rebuild:
        if not index_exists:
            print("ğŸ“ ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼Œå»ºç«‹æ–°çš„ RAG ç´¢å¼•...")
        else:
            print("ğŸ”„ åµæ¸¬åˆ°æª”æ¡ˆè®ŠåŒ–ï¼Œé‡å»º RAG ç´¢å¼•...")
            # æ¸…ç©ºèˆŠçš„ç´¢å¼•ç›®éŒ„
            if persist_dir.exists():
                import shutil
                shutil.rmtree(persist_dir)
        
        # é‡æ–°å»ºç«‹ç´¢å¼•
        print("ğŸ“– æ­£åœ¨è®€å–æ–‡ä»¶...")
        docs = SimpleDirectoryReader(
            str(DATA_DIR), 
            encoding="utf-8", 
            errors="ignore"
        ).load_data()
        
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(docs)} å€‹æ–‡ä»¶")
        
        print("ğŸ”ª æ­£åœ¨åˆ‡åˆ†æ–‡æœ¬å¡Š...")
        nodes = SentenceSplitter(
            chunk_size=800, 
            chunk_overlap=120
        ).get_nodes_from_documents(docs)
        
        print(f"âœ… ç”Ÿæˆ {len(nodes)} å€‹æ–‡æœ¬å¡Š")
        
        print("ğŸš€ æ­£åœ¨å»ºç«‹å‘é‡ç´¢å¼•...")
        index = VectorStoreIndex(nodes)
        
        print("ğŸ’¾ å„²å­˜ç´¢å¼•åˆ°ç£ç¢Ÿ...")
        index.storage_context.persist(persist_dir=str(persist_dir))
        
        print("âœ… ç´¢å¼•å»ºç«‹å®Œæˆï¼")
        
    else:
        print("âš¡ æ²’æœ‰æª”æ¡ˆè®ŠåŒ–ï¼Œè¼‰å…¥ç¾æœ‰ç´¢å¼•...")
        storage_ctx = StorageContext.from_defaults(persist_dir=str(persist_dir))
        index = load_index_from_storage(storage_ctx)
        print("âœ… ç´¢å¼•è¼‰å…¥å®Œæˆï¼")
    
    return index

# åŸ·è¡Œæ™ºèƒ½ç´¢å¼•ç®¡ç†
index = create_or_load_index()

qe = index.as_query_engine(similarity_top_k=5, response_mode="compact", text_qa_template=qa_tmpl)
print(qe.query("é€€æ›è²¨å¯ä»¥å¹¾å¤©è¾¦ï¼Ÿ").response)